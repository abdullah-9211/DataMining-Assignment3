{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "normalized_data = (data - data.mean()) / data.std()\n",
    "normalized_data = np.expand_dims(normalized_data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_geometric_mask(data, p=0.1):\n",
    "    mask = np.random.geometric(p, size=data.shape) > 1\n",
    "    return data * mask\n",
    "\n",
    "augmented_data = apply_geometric_mask(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132481, 26, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_autoencoder(input_shape):\n",
    "    # Encoder\n",
    "    input_layer = keras.Input(shape=input_shape)\n",
    "    x = layers.Normalization()(input_layer)\n",
    "    x = layers.MultiHeadAttention(num_heads=2, key_dim=2)(x, x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    encoded = layers.Dense(64, activation='relu')(x)\n",
    "    encoder = keras.Model(inputs=input_layer, outputs=encoded, name=\"encoder\")\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = keras.Input(shape=(64,))\n",
    "    x = layers.RepeatVector(input_shape[0])(decoder_input)\n",
    "    x = layers.MultiHeadAttention(num_heads=2, key_dim=2)(x, x)\n",
    "    x = layers.TimeDistributed(layers.Dense(input_shape[-1]))(x)\n",
    "    decoder = keras.Model(inputs=decoder_input, outputs=x, name=\"decoder\")\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder_output = decoder(encoder(input_layer))\n",
    "    autoencoder = keras.Model(inputs=input_layer, outputs=autoencoder_output, name=\"autoencoder\")\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "input_shape = (normalized_data.shape[1], normalized_data.shape[2])  # Modify as per your actual input shape\n",
    "autoencoder, encoder, decoder = build_autoencoder(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 26, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " normalization_2 (Normalization  (None, 26, 1)       3           ['input_7[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 26, 1)       29          ['normalization_2[0][0]',        \n",
      " eadAttention)                                                    'normalization_2[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 1)           0           ['multi_head_attention_4[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64)           128         ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 160\n",
      "Trainable params: 157\n",
      "Non-trainable params: 3\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generator\n",
    "generator = encoder\n",
    "\n",
    "# Discriminator\n",
    "discriminator = keras.Sequential([\n",
    "    keras.Input(shape=64),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "discriminator.trainable = False\n",
    "combined_input = keras.Input(shape=input_shape)\n",
    "combined_output = discriminator(generator(combined_input))\n",
    "combined_model = keras.Model(inputs=combined_input, outputs=combined_output)\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1664 into shape (64,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 26\u001b[0m\n\u001b[0;32m     20\u001b[0m             g_loss \u001b[38;5;241m=\u001b[39m combined_model\u001b[38;5;241m.\u001b[39mtrain_on_batch(noise, np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Discriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Generator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 6\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(generator, discriminator, combined_model, data, epochs, batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Sampling and generating data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size)\n\u001b[1;32m----> 6\u001b[0m     real_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Correct reshaping to (batch_size, 64)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Generate noise for the generator\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, generator\u001b[38;5;241m.\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1664 into shape (64,64)"
     ]
    }
   ],
   "source": [
    "def train_gan(generator, discriminator, combined_model, data, epochs=100, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(data.shape[0] // batch_size):\n",
    "            # Sampling and generating data\n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            real_data = data[idx].reshape(batch_size, 64)  # Correct reshaping to (batch_size, 64)\n",
    "\n",
    "            # Generate noise for the generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, generator.input_shape[1]))\n",
    "\n",
    "            # Generate fake data from noise\n",
    "            fake_data = generator.predict(noise)\n",
    "\n",
    "            # Discriminator training on real and fake data\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((batch_size, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Generator training to fool the discriminator\n",
    "            g_loss = combined_model.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "train_gan(generator, discriminator, combined_model, augmented_data, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4141/4141 [==============================] - 16s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "reconstructions = autoencoder.predict(normalized_data)\n",
    "losses = np.mean(np.square(normalized_data - reconstructions), axis=-1)\n",
    "threshold = np.percentile(losses, 95)\n",
    "anomalies = losses > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
